#pragma once

#include "executor.h"
#include "instgen.h"
#include "onnx_parser.h"
#include "tensor.h"
#include "instructions.h"
#include <cstddef>
#include <dlfcn.h>
#include <filesystem>
#include <string>

#define RAH_SO_STRING "librah.so"

// For UART Server
#define HANDLER_PORT 5001

namespace fs = std::filesystem;

/* Why re-invent streams?
 * stl streams do everything this does but extracting
 * a char* from them (so that it can be sent to librah)
 * leads to redundant copies. Fstream is a gentle cpp
 * wrapper around fread(), provides easy and cheap access
 * to the underlying char*.
 */
class Fstream {
  char *m_buf;
  size_t m_size;

public:
  Fstream(const std::string &filename);
  ~Fstream();
  const char *get_data() const;
  size_t get_size() const;
};

#define META_WIDTH_BITS 48
#define META_WIDTH_BYTES (META_WIDTH_BITS / 8)

std::vector<char> cvt_32248(int v);

class Rah {
public:
  virtual int write(const char *data, size_t size) = 0;
  virtual int read(char *data, size_t size) = 0;
  virtual void check_version() = 0;
  virtual ~Rah() {}
  virtual int write_meta(const std::bitset<META_WIDTH_BITS> type,
                         const std::vector<char> &data) = 0;
};

class RealRah : public Rah {
  void *m_handle;

public:
  RealRah();
  ~RealRah();
  int write(const char *data, size_t size) override;
  int read(char *data, size_t size) override;
  void check_version() override;
  int write_meta(const std::bitset<META_WIDTH_BITS> type,
                 const std::vector<char> &data) override;
};

/* Used as a dupe of rah while running gaticc on devices
 * that do not have rah (non-vaaman devices). Used by
 * the --dry-run option, mainly for debugging
 */
class FakeRah : public Rah {
  int write(const char *data, size_t size) override;
  int read(char *data, size_t size) override;
  int write_meta(const std::bitset<META_WIDTH_BITS> type,
                 const std::vector<char> &data) override;
  void check_version() override;
};

class AirRah : public Rah {
  int m_sock;
  void serv_send(int app_id, const char *data, int size);
public:
  AirRah(const std::string& server_ip);
  ~AirRah();
  int write(const char *data, size_t size) override;
  int read(char *data, size_t size) override;
  void check_version() override;
  int write_meta(const std::bitset<META_WIDTH_BITS> type,
                 const std::vector<char> &data) override;
};

/* Like dispatch table, but reads binarized instructions and creates a table
 * of hashed values. Used by Runner that has access to the graph, to match
 * hashed values with names from which the hash was generated.
 */
class HashedDispatchTable {
  std::vector<int> tbl;

public:
  HashedDispatchTable(const Fstream &fp);
  bool should_dispatch(const Op::LayerBase *l) const;
};

class Runner {
  TensorPool tensor_pool;
  Op::Parser *m_parser;

  void scan(Rah &rah);
  void device_init();
  void load_model(Rah &rah, const Fstream &fp);
  void tensor_pool_init();
  std::string get_run_arg();

  template <typename inputT, typename DeviceOutputT,
            typename OutputT>
  void run(Rah &rah, HashedDispatchTable &hdt);

  template <typename inputT>
  TensorPool infer_aux(Rah &rah, HashedDispatchTable &hdt, Tensor<inputT>* arr);

  void receive_output(Rah &rah, const Op::LayerBase *l, bool is_last_layer);
  void fake_exec(Op::LayerBase *l);
  void read_uart(BinBlob &blob, std::string handler_ip, int expected_size);

  template <typename T>
  void receive_output_aux(const T *data, const Op::LayerBase *l, bool is_last_layer);

public:
  Runner();
  TensorPool infer(const std::string& onnx_path, const std::string& gml_path, py::array arr);
};

/* infer_aux is a state-machine that passes through these states of execution:
 *
 * CPU-UNSENT
 *    inputs are present on the CPU, they should be passed through
 *    nodes with DEVICE == DEVICE_CPU
 * FPGA-UNSENT
 *    the first non-cpu node marks the dispatch of the outputs of
 *    the last cpu node to the FPGA. this branch is only taken
 *    once, to send the inputs to the FPGA
 * FPGA-SENT
 *    inputs meant for the FPGA have been sent and all nodes following
 *    this with DEVICE == DEVICE_FPGA should be fake executed
 * CPU-SENT
 *    after FPGA-SENT, first DEVICE_CPU node is the node that receives
 *    outputs generated by the FPGA. This node re-aligns the tensors
 *    (if necessary) and loads the received tensor into tensor_pool
 *    to continue rest of the execution on CPU.
 * CPU-CONTINUE
 *    all the other nodes following this are meant to be executed
 *    on CPU normally, at the end, the final output is to be sent
 *    to the pre-processing pipeline.
 */
template <typename inputT>
TensorPool Runner::infer_aux(Rah &rah, HashedDispatchTable &hdt, Tensor<inputT>* arr) {
  auto graph = m_parser->get_graph();
  auto order = Pass::remove_dqxq(graph);

  Op::LayerBase *last_layer = Op::get_last_layer(*m_parser);
  last_layer->dispatch = hdt.should_dispatch(last_layer);
  bool is_last_layer = true;
  Pass::extract_conv_true_odims(graph);
  AddressGen generator(graph);

  InstGen instgen(*m_parser);
  IOAddrTbl io_addr_tbl = instgen.get_io_addr_tbl();

  Op::RegisterAllocator allcator(m_parser->get_graph());

  Tensor<inputT> *input_image = arr;
  if (input_image->dims_size() <= 1) {
    log_fatal("Expects input images to be greater than 1 dimensional (N,...) "
              "got a {} dimensional "
              "image\n",
              input_image->dims_size());
  }
  log_info("preprocess finish\n");
  TensorPool ret;
  Timer<std::chrono::milliseconds> tt;
  tt.start();
  for (int i = 0; i < input_image->dims_at(0); ++i) {
    tensor_pool.free();

    Timer<std::chrono::microseconds> slice_tt;
    slice_tt.start();
    Tensor<inputT> *slice{get_slice(input_image, std::vector<int>{i})};
    print_vec("slice shape ", slice->get_dims());
    slice_tt.stop();
    log_info("Slice time {} us\n", slice_tt.difference().count());
    if (order.at(0)->input_dims[0] != slice->get_dims()) {
      log_fatal("Expected input dims {}, got input of dimensions {}\n",
                order.at(0)->input_dims[0], slice->get_dims());
    }
    tensor_pool.set<Tensor<inputT> *>(0, slice);

    bool dump_and_exit = false;
    bool sent = false;
    for (Op::LayerBase *l : order) {
      assert(l->device != DEVICE_UNKNOWN);

      l->dispatch = hdt.should_dispatch(l);
      log_info("Running layer {} on {}\n", l->name,
               Op::get_device_name(l->device));
      if (l->device == DEVICE_CPU && sent == false) {
        Timer<std::chrono::microseconds> run_tt;
        run_tt.start();
        l->run(tensor_pool);
        run_tt.stop();
        log_info("CPU Proc run: {} us\n", run_tt.difference().count());
      } 
      if (l->device == DEVICE_FPGA && sent == false) {
        Timer<std::chrono::microseconds> run_tt;
        run_tt.start();
        l->send_input(tensor_pool, generator, rah, io_addr_tbl);
        sent = true;
        run_tt.stop();
        log_info("Send Input time: {} us\n", run_tt.difference().count());
      } 
      if (l->device == DEVICE_FPGA && sent == true) {
        if (l->dispatch) {
          log_info("l->dispatch {} for layer {}\n", l->dispatch,
                   l->name.c_str());
          if (l->name != last_layer->name) {
            is_last_layer = false;
          }
          log_info("receiving output\n");
          Timer<std::chrono::microseconds> run_tt;
          run_tt.start();
          receive_output(rah, l, is_last_layer);
          run_tt.stop();
          log_info("Receive output time: {} us\n", run_tt.difference().count());
          log_info("receiving output finish\n");
          if (!last_layer->dispatch) {
            dump_and_exit = true;
            goto _dump;
          }
        } else {
          fake_exec(l);
        }
      } 
      if (l->device == DEVICE_CPU && sent == true) {
        if (last_layer->dispatch) {
          l->run(tensor_pool);
        } 
      }
_dump:
      if (m_parser->has_graph_output(l) || (l->dispatch && !(last_layer->dispatch && l->device == DEVICE_FPGA))) {
        for (auto type : l->output_type) {
          /* TODO: use unique_ptr */
          if (type == onnx::TensorProto_DataType_INT8) {
            Tensor<int8_t> *out =
                tensor_pool.get<Tensor<int8_t> *>(l->outputs.at(0));
            Tensor<int8_t> *out_copy = new TensorCreate(out, l->name);
            ret.push_back<Tensor<int8_t> *>(out_copy);
          } else if (type == onnx::TensorProto_DataType_FLOAT) {
            Tensor<float> *out =
                tensor_pool.get<Tensor<float> *>(l->outputs.at(0));
            Tensor<float> *out_copy = new TensorCreate(out, l->name);
            ret.push_back<Tensor<float> *>(out_copy);
          } else if (type == onnx::TensorProto_DataType_INT32) {
            Tensor<int> *out =
                tensor_pool.get<Tensor<int> *>(l->outputs.at(0));
            Tensor<int> *out_copy = new TensorCreate(out, l->name);
            ret.push_back<Tensor<int> *>(out_copy);
          } else {
            log_fatal("Output type of layer {} ({}) is not supported\n", l->name,
                      Op::get_tensorproto_dtype_name(type));
          }
        }
      }
      if (dump_and_exit) {
        break;
      }
    }
  }
  tt.stop();
  log_info("Infer loop, total time: {}ms\n", tt.difference().count());
  return ret;
}

template <typename T>
void unalign_sa_output(const Op::Layer::QLinearConv *l, Tensor<T> *tensor, const T *data) {
  assert(tensor->dims_size() == 4 && "Expected a 4 dimensional array (NCHW)");
  IVec2D og_dims_v {tensor->get_dims()};
  auto og_dims = og_dims_v.at(0);
  auto aligned_dims = aligned_conv_input_dims(og_dims_v, l->weights->dims())[0];
  auto sa_arch = get_sa_arch();
  int og_frame_sz = og_dims[TENSOR_4D_HEIGHT] * og_dims[TENSOR_4D_WIDTH];
  int frame_sz = aligned_dims[TENSOR_4D_HEIGHT] * aligned_dims[TENSOR_4D_WIDTH];
  int batch_size = aligned_dims[TENSOR_4D_CHANNELS] * frame_sz;
  int dk = WORD_SIZE / sa_arch[SA_ARCH_N];
  int data_index = 0;

  for (int b = 0; b < aligned_dims[TENSOR_4D_BATCH]; ++b) {
    for (int c = 0; c < aligned_dims[TENSOR_4D_CHANNELS] / sa_arch[SA_ARCH_N]; ++c) {
      for (int e = 0; e < ceil_mod(frame_sz, dk) / dk; ++e) {
        for (int ci = 0; ci < sa_arch[SA_ARCH_N]; ++ci) {
          for (int ei = 0; ei < dk; ++ei) {
            int chan_n = (c * sa_arch[SA_ARCH_N]) + ci;
            int elem_n = (e * dk) + ei;
            int index = (b * batch_size) + (chan_n * og_frame_sz) + elem_n;
            if (chan_n < og_dims[TENSOR_4D_CHANNELS] && elem_n < og_frame_sz) {
              tensor->set(index, data[data_index]);
            } 
            data_index++;
          }
        }
      }
    }
  }
}

template <typename T> void unalign_va_output(Tensor<T> *tensor, const T *data) {
  auto dims = tensor->get_dims();
  size_t size = prod(dims.begin(), dims.end(), 1);
  for (size_t i = 0; i < size; ++i) {
    T v = data[i];
    tensor->set(i, v);
  }
}

/* Converts a byte stream into a tensor and un-aligns if if necessary */
template <typename T>
void Runner::receive_output_aux(const T *data, const Op::LayerBase *l, bool is_last_layer) {
  static_assert(std::is_same<T, int8_t>() || std::is_same<T, uint8_t>());
  std::vector<int> odims = l->pipelined_output_dims.at(0);
  Tensor<T> *tensor = new TensorCreate<T>(odims);

  if (is_op_type(l, "QLinearConv") || is_op_type(l, "QLinearEltwise")) {
    log_warn("Dangerous type cast from QLinearEltwise to QLinearConv\n");
    unalign_sa_output(dynamic_cast<const Op::Layer::QLinearConv*>(l), tensor, data);
  } else if (is_op_type(l, "QGemm") || is_op_type(l, "Transpose")) {
    unalign_va_output(tensor, data);
  } else {
    log_fatal("cant handle un-alignment for layer of type {}\n", l->op_type());
  }

  log_info2("Unalign complete\n");
  if (tensor_pool.has_value(l->outputs.at(0))) {
    tensor_pool.free(l->outputs.at(0));
  }
  tensor_pool.set<Tensor<T> *>(l->outputs.at(0), tensor);

  if (l->dispatch) {
    pickle_tensor(tensor, "fpga_" + l->name);
  }
}

template <typename T> T get_dlsym(void *m_handle, std::string func_name) {
  T fn;
  fn = (T)dlsym(m_handle, func_name.c_str());
  char *error = dlerror();
  if (error != NULL) {
    log_fatal("dlsym: {}\n", error);
  }
  return fn;
}
